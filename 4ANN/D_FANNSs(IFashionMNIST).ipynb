{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI4IA_Assignment5_D",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWC_fWgxHrNO"
      },
      "source": [
        "# **Task D:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fssNpJdkHvot",
        "outputId": "a1e8b1cd-685b-4018-c4f7-0f63c4aef12f"
      },
      "source": [
        "## Code to access the FashionMNIST\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda, Compose\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import genfromtxt\n",
        "\n",
        "# Download training data from open datasets.\n",
        "training_data = datasets.FashionMNIST(\n",
        "root=\"data\",\n",
        "train=True,\n",
        "download=True,\n",
        "transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.FashionMNIST(\n",
        "root=\"data\",\n",
        "train=False,\n",
        "download=True,\n",
        "transform=ToTensor(),\n",
        ")\n",
        "print(training_data)\n",
        "print(test_data)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset FashionMNIST\n",
            "    Number of datapoints: 60000\n",
            "    Root location: data\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: ToTensor()\n",
            "Dataset FashionMNIST\n",
            "    Number of datapoints: 10000\n",
            "    Root location: data\n",
            "    Split: Test\n",
            "    StandardTransform\n",
            "Transform: ToTensor()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlkcMo8EIFKw",
        "outputId": "47bfe55d-bb8b-4311-c005-1d99bc38e97b"
      },
      "source": [
        "original_data = training_data\n",
        "size_of_original_data = len(original_data)\n",
        "print(size_of_original_data)\n",
        "\n",
        "\n",
        "train_fraction = 0.70\n",
        "val_fraction = 0.30\n",
        "\n",
        "train_dataset_size = int(train_fraction * size_of_original_data)\n",
        "val_dataset_size = int(val_fraction * size_of_original_data)\n",
        "\n",
        "# Split whole original data into train, val and test datsets\n",
        "train_dataset, val_dataset=  torch.utils.data.random_split(original_data,\n",
        "                                                           [train_dataset_size,\n",
        "                                                            val_dataset_size])\n",
        "# Sanity checking\n",
        "print(f\" Train set Size: {len(train_dataset)}\")\n",
        "print(f\" Val set Size: {len(val_dataset)}\")\n",
        "print(f\" test set Size: {len(test_data)}\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000\n",
            " Train set Size: 42000\n",
            " Val set Size: 18000\n",
            " test set Size: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0VQjg0IyX_-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fa016c4-00ee-4002-e637-b87a09fa517d"
      },
      "source": [
        "train_dataloader = DataLoader(dataset=train_dataset, batch_size=200, shuffle=True)\n",
        "val_dataloader= DataLoader(dataset=val_dataset, batch_size=200, shuffle=True)\n",
        "test_dataloader= DataLoader(dataset=test_data, batch_size=200, shuffle=True)\n",
        "print(train_dataloader)\n",
        "print(val_dataloader)\n",
        "print(test_dataloader)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<torch.utils.data.dataloader.DataLoader object at 0x7f0fc84ca250>\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7f0fc84ca1d0>\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7f0fc84ca0d0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-EXJvzY0PmC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "accdc59d-2167-49cd-8d05-37a8fe69f10d"
      },
      "source": [
        "# Simple way to define a Fully connected feed forward ANN\n",
        "'''\n",
        "model = nn.Sequential(\n",
        "nn.Linear(28*28,64),\n",
        "nn.ReLU(),\n",
        "nn.Linear(64,64),\n",
        "nn.ReLU(),\n",
        "nn.Linear(64,10)\n",
        ")\n",
        "\n",
        "\n",
        "nn.Flatten()'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nmodel = nn.Sequential(\\nnn.Linear(28*28,64),\\nnn.ReLU(),\\nnn.Linear(64,64),\\nnn.ReLU(),\\nnn.Linear(64,10)\\n)\\n\\n\\nnn.Flatten()'"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLt10o2EIeLr"
      },
      "source": [
        "# fexible model:\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ResNet, self).__init__()\n",
        "    self.l1=nn.Linear(28*28,300)\n",
        "    self.l2=nn.Linear(300,100)\n",
        "    self.output=nn.Linear(100,10)\n",
        "    \n",
        "\n",
        "    self.relu=nn.ReLU()\n",
        "    \n",
        "  def forward(self,x):\n",
        "    \n",
        "    h1=self.relu(self.l1(x))\n",
        "    #h2=nn.functional.relu(self.l2(h1))\n",
        "    h2=self.relu(self.l2(h1))\n",
        "    #h3 = self.relu(self.l2(h1))\n",
        "    output=self.output(h2)\n",
        "    #logits=self.l3(do)\n",
        "    return output\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HjEXYjj92q7"
      },
      "source": [
        "model= ResNet()\n",
        "optim = torch.optim.SGD(model.parameters(), lr=0.001)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e84YUhkg-LWX"
      },
      "source": [
        "cost_function = nn.CrossEntropyLoss()"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bDrccTmk927x",
        "outputId": "71884e9e-137b-4db2-963d-22ee593ea550"
      },
      "source": [
        "model=ResNet()\n",
        "optim = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "cost_function = nn.CrossEntropyLoss()\n",
        "\n",
        "training_minibatch_Js =[]\n",
        "training_minibatch_Js_mean=[]\n",
        "accuracy_list_1=[]\n",
        "\n",
        "nr_epochs = 500    # Try\n",
        "for epoch_i in range(nr_epochs):\n",
        "  for X_batch, y_batch in train_dataloader:\n",
        "    #print(X_batch)\n",
        "    \n",
        "    b= X_batch.size(0)\n",
        "    X_batch=X_batch.view(b,-1)\n",
        "\n",
        "    y_preds = model(X_batch)\n",
        "    cost = cost_function(y_preds, y_batch)\n",
        "\n",
        "    optim.zero_grad() # Set the grads of all model params to zero.\n",
        "\n",
        "    #Forward pass\n",
        "   \n",
        "\n",
        "    cost.backward() # compute and populate gradients of model params\n",
        "    \n",
        "    optim.step() # instruct optimizer to take one update step (using SGD)\n",
        "    \n",
        "    training_minibatch_Js.append(cost.item())\n",
        "    accuracy_list_1.append(y_batch.eq(y_preds.detach().argmax(dim=1)).float().mean())\n",
        "    #print(f'epoch {epoch_i},train cost:{training_minibatch_Js}')\n",
        "  print(f'epoch {epoch_i},train cost:{torch.tensor(training_minibatch_Js).mean():.4f}  &  train accuracies: {torch.tensor(accuracy_list_1).mean():.4f}')\n",
        "  \n",
        "  \n",
        "  val_track=[]\n",
        "  accuracy_list_2=[]\n",
        "  for batch in val_dataloader:\n",
        "    x, y = batch\n",
        "    b= x.size(0)\n",
        "    x=x.view(b,-1)\n",
        "    with torch.no_grad():\n",
        "      y_preds=model(x)\n",
        "    cost= cost_function(y_preds, y)\n",
        "    val_track.append(cost.item())\n",
        "    accuracy_list_2.append(y.eq(y_preds.detach().argmax(dim=1)).float().mean())\n",
        "  #print(f'Epoch {epoch_i},validation cost:{torch.tensor(val_track).mean():.4f}  &   validation accuracies: {torch.tensor(accuracy_list_2).mean():.4f}')\n",
        "\n",
        "# Plot your cost\n",
        "plt.figure(figsize=[10,5])\n",
        "plt.plot(training_minibatch_Js)\n",
        "plt.plot(val_track)\n",
        "plt.xlabel(f\"update step i on mini-batch\")\n",
        "plt.ylabel('Cost')\n",
        "plt.title('Cost during training on train-set (per mini-batch)')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0,train cost:2.3030  &  train accuracies: 0.0979\n",
            "epoch 1,train cost:2.3008  &  train accuracies: 0.1183\n",
            "epoch 2,train cost:2.2989  &  train accuracies: 0.1289\n",
            "epoch 3,train cost:2.2969  &  train accuracies: 0.1460\n",
            "epoch 4,train cost:2.2949  &  train accuracies: 0.1607\n",
            "epoch 5,train cost:2.2926  &  train accuracies: 0.1818\n",
            "epoch 6,train cost:2.2900  &  train accuracies: 0.1995\n",
            "epoch 7,train cost:2.2870  &  train accuracies: 0.2187\n",
            "epoch 8,train cost:2.2834  &  train accuracies: 0.2374\n",
            "epoch 9,train cost:2.2790  &  train accuracies: 0.2554\n",
            "epoch 10,train cost:2.2735  &  train accuracies: 0.2707\n",
            "epoch 11,train cost:2.2664  &  train accuracies: 0.2840\n",
            "epoch 12,train cost:2.2572  &  train accuracies: 0.2981\n",
            "epoch 13,train cost:2.2450  &  train accuracies: 0.3090\n",
            "epoch 14,train cost:2.2292  &  train accuracies: 0.3193\n",
            "epoch 15,train cost:2.2094  &  train accuracies: 0.3284\n",
            "epoch 16,train cost:2.1861  &  train accuracies: 0.3368\n",
            "epoch 17,train cost:2.1602  &  train accuracies: 0.3446\n",
            "epoch 18,train cost:2.1326  &  train accuracies: 0.3525\n",
            "epoch 19,train cost:2.1041  &  train accuracies: 0.3605\n",
            "epoch 20,train cost:2.0752  &  train accuracies: 0.3681\n",
            "epoch 21,train cost:2.0462  &  train accuracies: 0.3757\n",
            "epoch 22,train cost:2.0177  &  train accuracies: 0.3829\n",
            "epoch 23,train cost:1.9896  &  train accuracies: 0.3899\n",
            "epoch 24,train cost:1.9623  &  train accuracies: 0.3967\n",
            "epoch 25,train cost:1.9358  &  train accuracies: 0.4031\n",
            "epoch 26,train cost:1.9102  &  train accuracies: 0.4093\n",
            "epoch 27,train cost:1.8854  &  train accuracies: 0.4152\n",
            "epoch 28,train cost:1.8615  &  train accuracies: 0.4209\n",
            "epoch 29,train cost:1.8385  &  train accuracies: 0.4264\n",
            "epoch 30,train cost:1.8163  &  train accuracies: 0.4317\n",
            "epoch 31,train cost:1.7950  &  train accuracies: 0.4367\n",
            "epoch 32,train cost:1.7744  &  train accuracies: 0.4416\n",
            "epoch 33,train cost:1.7545  &  train accuracies: 0.4463\n",
            "epoch 34,train cost:1.7354  &  train accuracies: 0.4508\n",
            "epoch 35,train cost:1.7169  &  train accuracies: 0.4552\n",
            "epoch 36,train cost:1.6991  &  train accuracies: 0.4594\n",
            "epoch 37,train cost:1.6818  &  train accuracies: 0.4635\n",
            "epoch 38,train cost:1.6652  &  train accuracies: 0.4675\n",
            "epoch 39,train cost:1.6491  &  train accuracies: 0.4713\n",
            "epoch 40,train cost:1.6334  &  train accuracies: 0.4751\n",
            "epoch 41,train cost:1.6183  &  train accuracies: 0.4788\n",
            "epoch 42,train cost:1.6037  &  train accuracies: 0.4823\n",
            "epoch 43,train cost:1.5895  &  train accuracies: 0.4858\n",
            "epoch 44,train cost:1.5757  &  train accuracies: 0.4892\n",
            "epoch 45,train cost:1.5623  &  train accuracies: 0.4926\n",
            "epoch 46,train cost:1.5493  &  train accuracies: 0.4958\n",
            "epoch 47,train cost:1.5366  &  train accuracies: 0.4989\n",
            "epoch 48,train cost:1.5243  &  train accuracies: 0.5020\n",
            "epoch 49,train cost:1.5124  &  train accuracies: 0.5051\n",
            "epoch 50,train cost:1.5007  &  train accuracies: 0.5081\n",
            "epoch 51,train cost:1.4894  &  train accuracies: 0.5111\n",
            "epoch 52,train cost:1.4783  &  train accuracies: 0.5141\n",
            "epoch 53,train cost:1.4676  &  train accuracies: 0.5170\n",
            "epoch 54,train cost:1.4571  &  train accuracies: 0.5199\n",
            "epoch 55,train cost:1.4468  &  train accuracies: 0.5227\n",
            "epoch 56,train cost:1.4368  &  train accuracies: 0.5255\n",
            "epoch 57,train cost:1.4270  &  train accuracies: 0.5283\n",
            "epoch 58,train cost:1.4174  &  train accuracies: 0.5312\n",
            "epoch 59,train cost:1.4081  &  train accuracies: 0.5339\n",
            "epoch 60,train cost:1.3989  &  train accuracies: 0.5366\n",
            "epoch 61,train cost:1.3900  &  train accuracies: 0.5393\n",
            "epoch 62,train cost:1.3812  &  train accuracies: 0.5420\n",
            "epoch 63,train cost:1.3726  &  train accuracies: 0.5446\n",
            "epoch 64,train cost:1.3642  &  train accuracies: 0.5472\n",
            "epoch 65,train cost:1.3559  &  train accuracies: 0.5497\n",
            "epoch 66,train cost:1.3478  &  train accuracies: 0.5522\n",
            "epoch 67,train cost:1.3398  &  train accuracies: 0.5547\n",
            "epoch 68,train cost:1.3320  &  train accuracies: 0.5571\n",
            "epoch 69,train cost:1.3243  &  train accuracies: 0.5595\n",
            "epoch 70,train cost:1.3168  &  train accuracies: 0.5619\n",
            "epoch 71,train cost:1.3094  &  train accuracies: 0.5642\n",
            "epoch 72,train cost:1.3021  &  train accuracies: 0.5664\n",
            "epoch 73,train cost:1.2949  &  train accuracies: 0.5687\n",
            "epoch 74,train cost:1.2879  &  train accuracies: 0.5708\n",
            "epoch 75,train cost:1.2809  &  train accuracies: 0.5730\n",
            "epoch 76,train cost:1.2741  &  train accuracies: 0.5751\n",
            "epoch 77,train cost:1.2674  &  train accuracies: 0.5771\n",
            "epoch 78,train cost:1.2608  &  train accuracies: 0.5792\n",
            "epoch 79,train cost:1.2543  &  train accuracies: 0.5811\n",
            "epoch 80,train cost:1.2479  &  train accuracies: 0.5831\n",
            "epoch 81,train cost:1.2416  &  train accuracies: 0.5850\n",
            "epoch 82,train cost:1.2354  &  train accuracies: 0.5869\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-9725363a0842>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mX_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0my_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcost_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-9d6d6e5343e5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mh1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0mh2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPTdL53xr6W9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "8fe084a2-81a7-4334-f885-175dc296bcc2"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# spliting the dataset \n",
        "original_data = training_data\n",
        "size_of_original_data = len(original_data)\n",
        "print(f'Data set size: {size_of_original_data}')\n",
        "\n",
        "\n",
        "train_fraction = 0.70\n",
        "val_fraction = 0.30\n",
        "\n",
        "train_dataset_size = int(train_fraction * size_of_original_data)\n",
        "val_dataset_size = int(val_fraction * size_of_original_data)\n",
        "\n",
        "\n",
        "# Split whole original data into train, val and test datsets\n",
        "train_dataset, val_dataset=  torch.utils.data.random_split(original_data,\n",
        "                                                           [train_dataset_size,\n",
        "                                                            val_dataset_size])\n",
        "# Sanity checking\n",
        "print(f\" Train set Size: {len(train_dataset)}\")\n",
        "print(f\" Val set Size: {len(val_dataset)}\")\n",
        "print(f\" test set Size: {len(test_data)}\")\n",
        "\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(dataset=train_dataset, batch_size=200, shuffle=True)\n",
        "val_dataloader= DataLoader(dataset=val_dataset, batch_size=200, shuffle=True)\n",
        "test_dataloader= DataLoader(dataset=test_data, batch_size=200, shuffle=True)\n",
        "\n",
        "\n",
        "# fexible model:\n",
        "class ResNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ResNet, self).__init__()\n",
        "    self.l1=nn.Linear(28*28,300)\n",
        "    self.l2=nn.Linear(300,100)\n",
        "    self.output=nn.Linear(100,10)\n",
        "\n",
        "    self.relu=nn.ReLU()\n",
        "    self.sigmoid= nn.Sigmoid()\n",
        "    \n",
        "  def forward(self,x):\n",
        "    h1=self.relu(self.l1(x))\n",
        "    h2=self.relu(self.l2(h1))\n",
        "    output=self.output(h2)\n",
        "    return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model=ResNet()\n",
        "cost_function = nn.CrossEntropyLoss()\n",
        "optim = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "def evaluate_model_performance(dataset, model):\n",
        "  d_loader = DataLoader(dataset = dataset, batch_size=len(dataset))\n",
        "  cost_function=nn.CrossEntropyLoss()\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for x,y in d_loader:\n",
        "      raw_y_preds=model(x) \n",
        "    y_class_preds= raw_y_preds.argmax(dim=1) \n",
        "    eval_cost= cost_function(raw_y_preds, y).item()\n",
        "  model.train()\n",
        "  eval_acc=accuracy_score(y_pred=y_class_preds,y_true=y)\n",
        "\n",
        "  return eval_cost, eval_acc\n",
        "\n",
        "\n",
        "training_minibatch_Js=[]\n",
        "val_cost_l=[]\n",
        "val_acc_l=[]\n",
        "trian_cost_l=[]\n",
        "train_acc_l=[]\n",
        "\n",
        "nr_epochs=50\n",
        "for epoch_i in range(nr_epochs):\n",
        "  #if epoch_i%eval_every_kth ==0:\n",
        "  model.eval()\n",
        "  #print('Starting the frist epoc')\n",
        "  train_cost, train_acc= evaluate_model_performance(model=model, dataset = train_dataset) # train cost and accuracy on train_dataset\n",
        "  trian_cost_l.append(train_cost)\n",
        "  train_acc_l.append(train_acc)\n",
        "  print(f\" In Epoch No. {nr_epochs}:  Cost= {val_cost:.4f} &  Accuracy: {train_acc:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "  val_cost, val_acc= evaluate_model_performance(model=model, dataset = val_dataset)# validation cost and accuracy on val_dataset\n",
        "  val_cos_lt.append(val_cost)\n",
        "  val_acc_l.append(val_acc)\n",
        "  print(f\"In Epoch No. {nr_epochs}  Cost: {train_cost:.4f} & Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "  model.train()\n",
        "\n",
        "\n",
        "\n",
        "   \n",
        "    \n",
        "  training_minibatch_Js=[] \n",
        "  for image, label in train_dataloader:\n",
        "    label_pred = model(image)\n",
        "    cost = cost_function(label_pred , label)\n",
        "    optim.zero_grad() # Set the grads of all model params to zero.\n",
        "    cost.backward() # compute and populate gradients of model params\n",
        "    optim.step() # instruct optimizer to take one update step (using SGD)\n",
        "    \n",
        "    training_minibatch_Js.append(cost)\n",
        "    print(f' in epoch {epoch_i} the last cost of epoch is {training_minibatch_Js[-1]}')\n",
        "\n",
        "\n",
        "print(\"****************************\")\n",
        "print(f\"final cost : {training_minibatch_Js[-1] }\")\n",
        "\n",
        "# EVALUATE Model 'performance' on whole train and Validation AND TEST dataset\n",
        "# test_cost, test_acc= evaluate_model_performance(model=model, dataset = test_dataset)\n",
        "\n",
        "\n",
        "\n",
        "# ploting ###################################################33##################################3333\n",
        "# plotting cost \n",
        "plt.plot(training_minibatch_Js)\n",
        "plt.xlabel(f\"update step i on mini-batch\")\n",
        "plt.ylabel('Cost')\n",
        "plt.title('Cost during training on train-set (per mini-batch)')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# the average cross-entropy\n",
        "plt.plot(trian_cost, label='Train Set', color='b')\n",
        "plt.plot(val_cost, label='Validation Set', color='o')\n",
        "plt.title('Eval: Cross_entropy cost throughout training')\n",
        "plt.xlabel('epoches')\n",
        "plt.ylabel('Cross_entropy Cost')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Plot accuracy\n",
        "plt.plot(train_acc, label='Train Set', color='b')\n",
        "plt.plot(val_acc, label='Validation Set', color='o')\n",
        "plt.title('Accuracy performance throughout training')\n",
        "plt.xlabel('epoches')\n",
        "plt.ylabel(' Accuracy')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data set size: 60000\n",
            " Train set Size: 42000\n",
            " Val set Size: 18000\n",
            " test set Size: 10000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-752070529723>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m   \u001b[0;31m#print('Starting the frist epoc')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m   \u001b[0mtrain_cost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mevaluate_model_performance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# train cost and accuracy on train_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m   \u001b[0mtrian_cost_l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_cost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m   \u001b[0mtrain_acc_l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-752070529723>\u001b[0m in \u001b[0;36mevaluate_model_performance\u001b[0;34m(dataset, model)\u001b[0m\n\u001b[1;32m     68\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m       \u001b[0mraw_y_preds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0my_class_preds\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mraw_y_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0meval_cost\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mcost_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_y_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-752070529723>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mh1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0mh2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1176000x28 and 784x300)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLifxMohKPS0",
        "outputId": "454c1885-592d-49c6-f2a7-96a5233a3b0a"
      },
      "source": [
        " # Print all parameters in your neural network model\n",
        "for p in model.named_parameters():\n",
        "  print(p)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('l1.weight', Parameter containing:\n",
            "tensor([[-0.0195,  0.0015,  0.0030,  ..., -0.0120, -0.0301, -0.0336],\n",
            "        [-0.0116, -0.0162,  0.0065,  ..., -0.0204, -0.0110, -0.0328],\n",
            "        [-0.0181,  0.0030, -0.0338,  ..., -0.0348,  0.0010, -0.0149],\n",
            "        ...,\n",
            "        [ 0.0200, -0.0302,  0.0127,  ...,  0.0183, -0.0318,  0.0170],\n",
            "        [-0.0305, -0.0266,  0.0303,  ..., -0.0042, -0.0240, -0.0286],\n",
            "        [-0.0316, -0.0267,  0.0288,  ...,  0.0039,  0.0147, -0.0083]],\n",
            "       requires_grad=True))\n",
            "('l1.bias', Parameter containing:\n",
            "tensor([ 0.1310, -0.0285, -0.0162,  0.0870,  0.1320,  0.2093,  0.0885,  0.0007,\n",
            "        -0.0726,  0.1408,  0.1784,  0.0437, -0.1845,  0.1472,  0.1218,  0.0957,\n",
            "         0.1591, -0.0370, -0.0106,  0.1124, -0.1303,  0.0709,  0.0710,  0.0973,\n",
            "        -0.0430,  0.1865,  0.1030, -0.0508,  0.1641,  0.0191,  0.1128, -0.0211,\n",
            "        -0.0989,  0.3755,  0.1963,  0.1184,  0.0555,  0.1088,  0.1571,  0.1467,\n",
            "         0.1394,  0.0175,  0.0069, -0.0395,  0.0033,  0.0425,  0.0176,  0.0113,\n",
            "        -0.0130,  0.2609,  0.0864,  0.1496,  0.1399,  0.0992,  0.0781, -0.1009,\n",
            "        -0.0722, -0.0927, -0.0692,  0.0215,  0.2744, -0.2027, -0.0570,  0.0878],\n",
            "       requires_grad=True))\n",
            "('l2.weight', Parameter containing:\n",
            "tensor([[ 0.2114, -0.1592,  0.0590,  ..., -0.1584, -0.0049, -0.1080],\n",
            "        [ 0.0516, -0.0632, -0.0510,  ...,  0.1186, -0.0126,  0.0352],\n",
            "        [ 0.0225,  0.0247,  0.0143,  ...,  0.1196, -0.0963,  0.0656],\n",
            "        ...,\n",
            "        [-0.0796, -0.0695,  0.0912,  ...,  0.1124,  0.1354, -0.0449],\n",
            "        [ 0.1774, -0.1564,  0.0012,  ...,  0.0427,  0.0062,  0.0639],\n",
            "        [ 0.0446, -0.0147,  0.0003,  ...,  0.0429,  0.0744,  0.2823]],\n",
            "       requires_grad=True))\n",
            "('l2.bias', Parameter containing:\n",
            "tensor([ 0.1173, -0.0215,  0.0396,  0.1576, -0.1209,  0.1455,  0.2569, -0.0568,\n",
            "         0.0718,  0.0776, -0.0824, -0.0660, -0.0563,  0.0101,  0.1387,  0.1586,\n",
            "         0.2258, -0.0205, -0.0962,  0.0543, -0.0262, -0.0262, -0.0947, -0.0629,\n",
            "         0.0212,  0.0319,  0.0657,  0.0319, -0.0219,  0.0930,  0.1842,  0.0364,\n",
            "        -0.1493, -0.0267,  0.2352, -0.0481,  0.1110, -0.0260,  0.0372,  0.0575,\n",
            "         0.1218,  0.0752,  0.2049, -0.1148, -0.0372,  0.1112,  0.1881, -0.0170,\n",
            "         0.0445,  0.0080,  0.0803,  0.0656,  0.0229, -0.0049, -0.0025, -0.0194,\n",
            "        -0.0650, -0.1216,  0.0059,  0.1242,  0.0888, -0.2026,  0.1028,  0.0863],\n",
            "       requires_grad=True))\n",
            "('output.weight', Parameter containing:\n",
            "tensor([[ 0.0192,  0.0561,  0.1180,  0.0772, -0.2672, -0.1217, -0.0515, -0.1957,\n",
            "         -0.1226, -0.0981,  0.0106,  0.1420, -0.0550, -0.2688,  0.1790, -0.2574,\n",
            "         -0.3610, -0.1508, -0.1458, -0.1281,  0.1219, -0.1856, -0.0883, -0.0853,\n",
            "         -0.1576,  0.3460,  0.0278,  0.1977,  0.2560, -0.0616,  0.2036, -0.1241,\n",
            "         -0.0959,  0.2835,  0.2874,  0.1006,  0.0458, -0.0441,  0.1221,  0.2663,\n",
            "          0.1243,  0.0610,  0.2496,  0.0176,  0.0806, -0.2940,  0.1676,  0.1704,\n",
            "         -0.1919,  0.3078, -0.0920,  0.1330, -0.2321,  0.0813,  0.0561, -0.0398,\n",
            "         -0.1735,  0.2472, -0.1512,  0.0739,  0.0679,  0.1693,  0.2051,  0.1189],\n",
            "        [-0.3985,  0.4083,  0.1476, -0.0090, -0.3115, -0.1406, -0.0152,  0.0200,\n",
            "          0.0393, -0.1276, -0.0637,  0.2226,  0.2141,  0.2527,  0.1293, -0.2754,\n",
            "         -0.2379, -0.0360, -0.0627, -0.2685,  0.2035,  0.1432, -0.1880, -0.1714,\n",
            "         -0.1452, -0.1569,  0.0133, -0.1068,  0.0020,  0.1389,  0.0555,  0.0930,\n",
            "         -0.2171, -0.2583, -0.1989,  0.3238,  0.1021,  0.2937,  0.0970, -0.0427,\n",
            "         -0.3203,  0.1010, -0.2844,  0.0345,  0.1264, -0.2579, -0.0689, -0.0224,\n",
            "          0.1482, -0.1902,  0.0530,  0.4187,  0.0048, -0.0880,  0.1965, -0.0114,\n",
            "         -0.1950, -0.1129,  0.4792,  0.1699,  0.1522, -0.0787, -0.3975, -0.1403],\n",
            "        [ 0.1612,  0.1291,  0.1801, -0.0810, -0.1621,  0.3277,  0.1142,  0.1725,\n",
            "         -0.3078, -0.2979,  0.0325, -0.0487, -0.0014, -0.0483,  0.2323, -0.0973,\n",
            "         -0.1870, -0.0846,  0.0721,  0.1546,  0.1771,  0.2217, -0.0772, -0.3233,\n",
            "         -0.1823, -0.2449, -0.2000,  0.2231, -0.0810, -0.0095,  0.1076,  0.2244,\n",
            "         -0.0143,  0.2630, -0.1590, -0.2861,  0.0079, -0.1762, -0.1822,  0.2226,\n",
            "          0.1172, -0.1414,  0.1677,  0.0061, -0.0444, -0.0300, -0.1892,  0.1836,\n",
            "          0.0346, -0.3375,  0.1923,  0.0874,  0.3037, -0.1677, -0.0270, -0.1151,\n",
            "          0.2042, -0.2188,  0.0515,  0.1346, -0.1905, -0.2264,  0.0882,  0.2488],\n",
            "        [ 0.2918,  0.1415,  0.1207, -0.0446,  0.1256, -0.0344, -0.1987, -0.0623,\n",
            "          0.2521, -0.1000, -0.1204,  0.0699, -0.2298, -0.3263,  0.2529,  0.0128,\n",
            "          0.1883,  0.1551, -0.2175,  0.1239,  0.0995, -0.0918,  0.1364, -0.2732,\n",
            "         -0.1699, -0.1404, -0.0268,  0.0512,  0.0457, -0.1233, -0.2621,  0.0354,\n",
            "         -0.0376,  0.0224, -0.1666,  0.1699, -0.0092,  0.1697, -0.2166,  0.0020,\n",
            "         -0.1418,  0.1659,  0.1992,  0.0345,  0.0100,  0.0035,  0.1418,  0.1348,\n",
            "          0.2876,  0.1280, -0.1630, -0.1875,  0.1431, -0.1340,  0.1232, -0.1298,\n",
            "          0.1109, -0.2637,  0.0982, -0.1991,  0.2663, -0.2390,  0.1870, -0.1742],\n",
            "        [-0.1651,  0.1606,  0.1947,  0.0067,  0.2050, -0.1285, -0.1430,  0.0414,\n",
            "         -0.1469, -0.3910,  0.1745, -0.2405,  0.2629,  0.1877,  0.0989,  0.0408,\n",
            "          0.2229,  0.1994,  0.1298,  0.1243,  0.1544,  0.2073, -0.1402, -0.0262,\n",
            "         -0.1191,  0.0662, -0.1188,  0.1992, -0.3202, -0.1243, -0.3329, -0.2005,\n",
            "          0.2304,  0.0600, -0.3839, -0.1841,  0.0394,  0.0463,  0.0243, -0.3140,\n",
            "          0.1277,  0.0689, -0.1411, -0.0208, -0.0955, -0.0873, -0.2003,  0.2045,\n",
            "          0.3261, -0.0763,  0.1296, -0.0586, -0.3297,  0.0307,  0.1160,  0.1685,\n",
            "          0.1454, -0.3252,  0.0996,  0.0414, -0.2609,  0.0965,  0.1840,  0.2747],\n",
            "        [-0.0510, -0.3444, -0.0870,  0.0979,  0.1768,  0.2826,  0.4334, -0.0463,\n",
            "         -0.2315,  0.3254,  0.2581, -0.2146, -0.1729,  0.1419, -0.2573,  0.1647,\n",
            "          0.2503, -0.1698, -0.0948, -0.0468, -0.5405,  0.0567,  0.2141,  0.2349,\n",
            "          0.1329,  0.2783,  0.3684, -0.3395, -0.0379, -0.1086,  0.2887, -0.2097,\n",
            "         -0.2971,  0.2122,  0.3371, -0.0094,  0.2153,  0.1138,  0.3571,  0.0790,\n",
            "          0.1399,  0.0464, -0.0171, -0.0015, -0.0419,  0.2570,  0.1112, -0.0711,\n",
            "          0.0265,  0.1847,  0.1197, -0.0340,  0.1805,  0.2908, -0.1731, -0.3165,\n",
            "         -0.3690, -0.2613, -0.2934,  0.0588,  0.2312, -0.1789, -0.4127,  0.0216],\n",
            "        [ 0.0157,  0.0804,  0.1331, -0.2262, -0.4061, -0.1599, -0.0770, -0.0800,\n",
            "         -0.2675, -0.3603, -0.0753,  0.0744, -0.2207,  0.3245, -0.0207,  0.0853,\n",
            "          0.3925, -0.0131,  0.0785,  0.1476,  0.1485,  0.1257, -0.4194, -0.0574,\n",
            "          0.0945,  0.0330,  0.1698,  0.1390,  0.1688,  0.1791, -0.3105, -0.2077,\n",
            "          0.2155,  0.1713,  0.2390,  0.0012, -0.0656, -0.3040,  0.2481,  0.1137,\n",
            "          0.0698, -0.0546,  0.2621,  0.0979,  0.0721, -0.1705,  0.2790,  0.1457,\n",
            "         -0.1418,  0.2556, -0.1867,  0.0373,  0.0586, -0.0879, -0.0716,  0.2897,\n",
            "          0.1863,  0.2353, -0.2033, -0.2697,  0.1297, -0.1867,  0.1847,  0.2491],\n",
            "        [-0.1474, -0.3367, -0.2562,  0.0666,  0.2694,  0.1528, -0.0549,  0.0775,\n",
            "          0.3196,  0.3440, -0.0857, -0.1178,  0.1219, -0.0677,  0.1589,  0.1185,\n",
            "          0.2440, -0.2135,  0.1951,  0.0222, -0.2862, -0.2960,  0.1604,  0.1445,\n",
            "          0.1608,  0.0589,  0.0739, -0.3994,  0.2716, -0.0576, -0.1028,  0.1525,\n",
            "         -0.0539, -0.1683, -0.1949, -0.2204,  0.2259,  0.0142, -0.1796, -0.2955,\n",
            "         -0.0008, -0.2489,  0.0258, -0.0893,  0.0854,  0.2538, -0.0468, -0.4137,\n",
            "         -0.1064, -0.2203,  0.2294,  0.1522,  0.2107, -0.1938,  0.2665,  0.0258,\n",
            "         -0.0027,  0.2072,  0.1508,  0.1279, -0.2782,  0.1722,  0.1036, -0.5638],\n",
            "        [ 0.1559, -0.1983,  0.2590,  0.0742,  0.3152, -0.3426, -0.0151,  0.1342,\n",
            "          0.2511,  0.1712, -0.1373, -0.1932,  0.1752, -0.2706, -0.2502,  0.0609,\n",
            "         -0.3317, -0.1066,  0.1364,  0.1079,  0.0440,  0.0209, -0.0343,  0.1061,\n",
            "          0.0858, -0.1791, -0.2866, -0.0636, -0.0946,  0.3652,  0.2271, -0.0030,\n",
            "          0.0403, -0.0639,  0.2606, -0.0118, -0.2822, -0.1906,  0.1972, -0.1645,\n",
            "         -0.0219, -0.2568, -0.1697,  0.0969,  0.1550,  0.1998, -0.1167,  0.1876,\n",
            "         -0.3479,  0.0880,  0.2327,  0.0739, -0.2764,  0.0110,  0.0416, -0.2324,\n",
            "          0.0517,  0.2608, -0.3583, -0.1788,  0.2059,  0.3737,  0.1064,  0.1948],\n",
            "        [-0.0687, -0.1779, -0.4324, -0.1915,  0.0108, -0.1132, -0.3600, -0.2194,\n",
            "          0.1560,  0.3602, -0.1839,  0.1081, -0.1136, -0.1852, -0.3001,  0.1605,\n",
            "         -0.1335,  0.0163, -0.2472, -0.2482, -0.0294, -0.2479,  0.2713,  0.2268,\n",
            "          0.1896, -0.1980,  0.3365,  0.0966,  0.0423, -0.1710,  0.1207,  0.1453,\n",
            "          0.2410, -0.3471, -0.1237,  0.0212,  0.1499, -0.0055, -0.2840,  0.1886,\n",
            "          0.0242, -0.1304, -0.0210, -0.0889,  0.0396,  0.2852, -0.1556, -0.1793,\n",
            "          0.2241, -0.2143, -0.2488, -0.2571,  0.1858, -0.0446, -0.4336,  0.1515,\n",
            "         -0.0175,  0.2328,  0.0885,  0.0739, -0.2187,  0.2084, -0.0965, -0.1094]],\n",
            "       requires_grad=True))\n",
            "('output.bias', Parameter containing:\n",
            "tensor([ 0.1381,  0.1225,  0.0874,  0.1693, -0.2399,  0.4515,  0.1057, -0.1112,\n",
            "        -0.2438, -0.4414], requires_grad=True))\n"
          ]
        }
      ]
    }
  ]
}